{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn BN Spouse\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018952131271362305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384b59568fab4289aa0af20b2da3dfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn BN Child\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03225135803222656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42cadaacf804c749c97939443fa8883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn BN Grandchild\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01944112777709961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6253da43539c46b8bf6cc4ec8a648e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn BN Others\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020322322845458984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46629cc02df748039c4c29bfa78d2f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025541067123413086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84582c01a9d4243acebe002360cf6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age_main sex_main     persinc_main nolicence_main anywork_main age_Spouse  \\\n",
      "0     100+        M    $400-599 p.w.   Some Licence            N      70-79   \n",
      "1     100+        M      $2000+ p.w.   Some Licence            Y      40-49   \n",
      "2     100+        M  Negative Income   Some Licence            Y       100+   \n",
      "3     100+        M  $1500-1999 p.w.   Some Licence            Y      50-59   \n",
      "4     100+        M    $200-299 p.w.   Some Licence            Y      90-99   \n",
      "5     100+        F    $800-999 p.w.   Some Licence            Y      70-79   \n",
      "6     100+        M    $200-299 p.w.     No Licence            N      40-49   \n",
      "7     100+        M  $1500-1999 p.w.   Some Licence            N      70-79   \n",
      "8     100+        M      $2000+ p.w.   Some Licence            N        0-9   \n",
      "9     100+        M    $600-799 p.w.   Some Licence            N      60-69   \n",
      "\n",
      "  sex_Spouse   persinc_Spouse nolicence_Spouse anywork_Spouse  \n",
      "0          F      Zero Income       No Licence              N  \n",
      "1          F  $1500-1999 p.w.     Some Licence              Y  \n",
      "2          F    $400-599 p.w.       No Licence              N  \n",
      "3          F      $1-199 p.w.     Some Licence              N  \n",
      "4          F    $200-299 p.w.     Some Licence              N  \n",
      "5          M  $1000-1249 p.w.     Some Licence              Y  \n",
      "6          F    $200-299 p.w.     Some Licence              N  \n",
      "7          F      $2000+ p.w.     Some Licence              N  \n",
      "8          F      $1-199 p.w.     Some Licence              N  \n",
      "9          F      Zero Income       No Licence              N  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from process_data import HH_ATTS, PP_ATTS, ALL_RELA\n",
    "from PopSynthesis.Methods.BN.utils.learn_BN import learn_struct_BN_score, learn_para_BN\n",
    "from pgmpy.sampling import BayesianModelSampling\n",
    "from pgmpy.factors.discrete import State\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "import pickle\n",
    "\n",
    "\n",
    "def learn_para_BN_new(model, data_df, state_names):\n",
    "    para_learn = BayesianEstimator(\n",
    "            model=model,\n",
    "            data=data_df,\n",
    "            state_names=state_names\n",
    "        )\n",
    "    ls_CPDs = para_learn.get_parameters(\n",
    "        prior_type='K2'\n",
    "    )\n",
    "    model.add_cpds(*ls_CPDs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def process_combine_df(combine_df):\n",
    "    combine_df[\"hhid\"] = combine_df.index\n",
    "    hh_df = combine_df[HH_ATTS]\n",
    "    all_rela_exist = ALL_RELA.copy()\n",
    "    all_rela_exist.remove(\"Self\")\n",
    "    hh_df[\"hhsize\"] = combine_df[all_rela_exist].sum(axis=1)\n",
    "    pp_cols = PP_ATTS + all_rela_exist\n",
    "    pp_cols.remove(\"relationship\")\n",
    "    pp_cols.remove(\"persid\")\n",
    "    pp_df = combine_df[pp_cols]\n",
    "    return hh_df, pp_df\n",
    "\n",
    "\n",
    "def extra_pp_df(pp_df):\n",
    "    to_drop_cols = [x  for x in pp_df.columns if x in ALL_RELA]\n",
    "    pp_df = pp_df.drop(columns=to_drop_cols)\n",
    "    pp_df[\"relationsip\"] = \"Self\"\n",
    "    return pp_df\n",
    "\n",
    "\n",
    "def get_2_pp_connect_state_names(state_names_base, rela):\n",
    "    new_dict_name = {}\n",
    "    for name in state_names_base:\n",
    "        new_dict_name[f\"{name}_main\"] = state_names_base[name]\n",
    "        new_dict_name[f\"{name}_{rela}\"] = state_names_base[name]\n",
    "    return new_dict_name\n",
    "\n",
    "\n",
    "def inference_model_get(ls_rela, state_names_base):\n",
    "    re_dict = {}\n",
    "    for rela in ls_rela:\n",
    "        df = pd.read_csv(f\"../data/connect_main_{rela}.csv\")\n",
    "        id_cols = [x for x in df.columns if \"hhid\" in x or \"persid\" in x]\n",
    "        df = df.drop(columns=id_cols)\n",
    "        print(f\"Learn BN {rela}\")\n",
    "        rela_state_names = get_2_pp_connect_state_names(state_names_base, rela)\n",
    "        model = learn_struct_BN_score(df, show_struct=False, state_names=rela_state_names)\n",
    "        model = learn_para_BN_new(model, df, state_names=rela_state_names)\n",
    "        re_dict[rela] = BayesianModelSampling(model)\n",
    "    return re_dict\n",
    "\n",
    "\n",
    "def process_rela_connect(main_pp_df, infer_model, rela):\n",
    "    print(f\"Processing the relationship {rela}\")\n",
    "    # Loop through each HH and append\n",
    "    all_cols = [x for x in main_pp_df.columns if x not in ALL_RELA]\n",
    "    all_cols.remove(\"hhid\")\n",
    "    ls_df = []\n",
    "    for i, row in main_pp_df.iterrows():\n",
    "        if i % 100 == 0:\n",
    "            print(f\"DOING PROGRESS: {(i*100)/len(main_pp_df)}%\")\n",
    "        evidences = [State(f\"{name}_main\", row[name]) for name in all_cols]\n",
    "        print(evidences, rela)\n",
    "        syn = infer_model.rejection_sample(evidence=evidences, size=row[rela], show_progress=True)\n",
    "        remove_cols = [x for x in syn.columns if \"_main\" in x]\n",
    "        syn = syn.drop(columns=remove_cols)\n",
    "        if row[rela] > 0:\n",
    "            syn.columns = syn.columns.str.rstrip(f'_{rela}')\n",
    "            syn[\"relationship\"] = rela\n",
    "            syn[\"hhid\"] = row[\"hhid\"]\n",
    "            ls_df.append(syn)\n",
    "    re_df = pd.concat(ls_df)\n",
    "    return re_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Import the synthetic with main and households\n",
    "    combine_df = pd.read_csv(r\"..\\output\\SynPop_hh_main_POA.csv\")\n",
    "    # Process the HH and main to have the HH with IDs and People in HH\n",
    "    hh_df, main_pp_df_all = process_combine_df(combine_df)\n",
    "    # Store the HH in df, Store the main in a list to handle later\n",
    "    store_pp_df = extra_pp_df(main_pp_df_all)\n",
    "    ls_df_pp = [store_pp_df]\n",
    "\n",
    "    state_names_pp = None\n",
    "    with open('../data/dict_pp_states.pickle', 'rb') as handle:\n",
    "        state_names_pp = pickle.load(handle)\n",
    "\n",
    "    all_rela_exist = ALL_RELA.copy()\n",
    "    all_rela_exist.remove(\"Self\")\n",
    "\n",
    "    dict_model_inference = inference_model_get(all_rela_exist, state_names_pp)\n",
    "    infer_model = dict_model_inference[\"Spouse\"]\n",
    "    evidences = [State(f\"age_main\", \"100+\")]\n",
    "    syn = infer_model.rejection_sample(evidence=evidences, size=10, show_progress=True)\n",
    "    print(syn)\n",
    "\n",
    "\n",
    "if __name__ ==  \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popsyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
